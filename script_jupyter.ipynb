{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fab03f-86ad-4bde-aa03-eebe4227b769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e50ad56-9393-4b2c-b41d-391bc65f7f18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Inicialização de variáveis\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkJob\").getOrCreate()\n",
    "\n",
    "# Configurações do banco e dados\n",
    "driver = \"org.postgresql.Driver\"\n",
    "database_host = \"psql-mock-database-cloud.postgres.database.azure.com\"\n",
    "database_port = \"5432\"\n",
    "database_name = \"ecom1692290925429vkltgmdlpclbtrmh\"\n",
    "user = \"mttdnzukfyledzwzollrcydx@psql-mock-database-cloud\"\n",
    "password = \"anljjxxmzjlpbtvkvegkwnyt\"\n",
    "url = f\"jdbc:postgresql://{database_host}:{database_port}/{database_name}\"\n",
    "\n",
    "# Dicionário de Dados contendo o nome da tabela e a sua respectiva chave primária\n",
    "dict_table_key = {'customers': ['customer_number'], \n",
    "                  'employees': ['employee_number'], \n",
    "                  'offices': ['office_code'], \n",
    "                  'orderdetails': ['order_line_number', 'order_number'], \n",
    "                  'orders': ['order_number'], \n",
    "                  'payments': ['check_number'], \n",
    "                  'product_lines': ['product_line'], \n",
    "                  'products': ['product_code']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705de7ae-3b10-4cf0-a1df-00382aa0a57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Salvar cada tabela como parquet\n",
    "\n",
    "for table in dict_table_key.keys():\n",
    "    df = (spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"driver\", driver)\n",
    "            .option(\"url\", url)\n",
    "            .option(\"dbtable\", table)\n",
    "            .option(\"user\", user)\n",
    "            .option(\"password\", password)\n",
    "            .load()\n",
    "         )\n",
    "    \n",
    "    df.write.mode(\"overwrite\").parquet(\"/parquet/\" + table + \".parquet\")\n",
    "\n",
    "#df=spark.read.parquet(\"/parquet/product_lines.parquet\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652efbc9-615a-4435-b939-952f74d0b902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Merge entre a tabela JDBC e os arquivos parquet, o merge contém a lógica de insert, update e delete.\n",
    "\n",
    "for table, p_key in dict_table_key.items():\n",
    "    (spark.read\n",
    "         .format(\"jdbc\")\n",
    "         .option(\"driver\", driver)\n",
    "         .option(\"url\", url)\n",
    "         .option(\"dbtable\", table)\n",
    "         .option(\"user\", user)\n",
    "         .option(\"password\", password)\n",
    "         .load()\n",
    "         .createOrReplaceTempView(table + \"_jdbc\"))\n",
    "    \n",
    "    # O dado é replicado em delta porque o Databricks só executa o MERGE INTO nesse tipo de dado\n",
    "    spark.read.parquet(\"/parquet/\" + table + \".parquet\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/\" + table)\n",
    "    spark.read.load(\"/tmp/delta/\" + table).createOrReplaceTempView(table + \"_delta\")\n",
    "\n",
    "    # SQL dinâmico para cada tabela \n",
    "    lst_columns = spark.sql(\"select * from \" + table + \"_delta limit 1\").columns\n",
    "    str_set = ', '.join('TARGET.{0}=SOURCE.{0}'.format(c) for c in lst_columns)\n",
    "    str_columns = ', '.join(lst_columns)\n",
    "    str_key = ' AND '.join('TARGET.{0}=SOURCE.{0}'.format(c) for c in p_key)\n",
    "\n",
    "    sql = \"\"\"\n",
    "        MERGE INTO {0} AS TARGET\n",
    "        USING {1} AS SOURCE\n",
    "        ON {2}\n",
    "        WHEN MATCHED AND 1=1 THEN \n",
    "        UPDATE SET {3}\n",
    "        WHEN NOT MATCHED BY TARGET THEN\n",
    "        INSERT ({4})\n",
    "        VALUES ({4})\n",
    "        WHEN NOT MATCHED BY SOURCE THEN\n",
    "        DELETE\n",
    "        \"\"\".format(table + \"_delta\", table + \"_jdbc\", str_key, str_set, str_columns)\n",
    "\n",
    "    spark.sql(sql)\n",
    "\n",
    "    #Salva o resultado no parquet\n",
    "    spark.sql(\"select * from \" + table + \"_delta\").write.mode(\"overwrite\").parquet(\"/parquet/\" + table + \".parquet\")\n",
    "\n",
    "    #exclui a tabela delta temporária\n",
    "    dbutils.fs.rm(\"/tmp/delta/\" + table,recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf423647-7202-42b3-bb36-0c5682bb5963",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n|qtde_cancelados|country|\n+---------------+-------+\n|            605|  Spain|\n+---------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#### Análise de dados para responder as seguintes perguntas e salvar o resultado em formato delta\n",
    "\n",
    "# Qual país possui a maior quantidade de itens cancelados?\n",
    "spark.read.parquet(\"/parquet/orders.parquet\").createOrReplaceTempView(\"orders\")\n",
    "spark.read.parquet(\"/parquet/customers.parquet\").createOrReplaceTempView(\"customers\")\n",
    "spark.read.parquet(\"/parquet/orderdetails.parquet\").createOrReplaceTempView(\"orderdetails\")\n",
    "\n",
    "sql = \"\"\"\n",
    "    select sum(ord_det.quantity_ordered) qtde_cancelados, country \n",
    "      from orders ord\n",
    "inner join customers cus on cus.customer_number = ord.customer_number\n",
    "inner join orderdetails ord_det on ord_det.order_number = ord.order_number\n",
    "     where ord.status = 'Cancelled'\n",
    "  group by country\n",
    "  order by 1 desc\n",
    "     limit 1\n",
    "\"\"\"     \n",
    "\n",
    "spark.sql(sql).show()\n",
    "spark.sql(sql).write.format(\"delta\").mode(\"overwrite\").save(\"/delta/result_pais_cancelados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77ed4c7-0c41-4620-a0ea-861b8171f548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n|product_line|faturamento|\n+------------+-----------+\n|Classic Cars|  603666.99|\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Qual o faturamento da linha de produto mais vendido, considere como os itens Shipped, cujo o pedido foi realizado no ano de 2005?\n",
    "spark.read.parquet(\"/parquet/product_lines.parquet\").createOrReplaceTempView(\"product_lines\")\n",
    "spark.read.parquet(\"/parquet/products.parquet\").createOrReplaceTempView(\"products\")\n",
    "spark.read.parquet(\"/parquet/orders.parquet\").createOrReplaceTempView(\"orders\")\n",
    "spark.read.parquet(\"/parquet/orderdetails.parquet\").createOrReplaceTempView(\"orderdetails\")\n",
    "\n",
    "sql = \"\"\"\n",
    "    select pl.product_line, sum(od.quantity_ordered * od.price_each) as faturamento \n",
    "      from product_lines pl\n",
    "inner join products p on p.product_line = pl.product_line\n",
    "inner join orderdetails od on od.product_code = p.product_code\n",
    "inner join orders o on o.order_number = od.order_number\n",
    "     where o.status = 'Shipped' and extract(year from o.order_date) = 2005\n",
    "group by pl.product_line\n",
    "order by 2 desc\n",
    "   limit 1\n",
    "\"\"\"     \n",
    "\n",
    "spark.sql(sql).show()\n",
    "spark.sql(sql).write.format(\"delta\").mode(\"overwrite\").save(\"/delta/result_faturamento_produto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166fe543-410a-49d4-8d6a-7a60ba1c0b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n|first_name|last_name|               email|\n+----------+---------+--------------------+\n|      Mami|    Nishi|########@classicm...|\n|   Yoshimi|     Kato|########@classicm...|\n+----------+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Nome, sobrenome e e-mail dos vendedores do Japão, o local-part do e-mail deve estar mascarado.\n",
    "spark.read.parquet(\"/parquet/employees.parquet\").createOrReplaceTempView(\"employees\")\n",
    "spark.read.parquet(\"/parquet/offices.parquet\").createOrReplaceTempView(\"offices\")\n",
    "\n",
    "sql = \"\"\"\n",
    "    select first_name, last_name, replace(email, split_part(email, '@', 1), '########') email\n",
    "      from employees emp\n",
    "inner join offices off on off.office_code = emp.office_code\n",
    "where off.country = 'Japan'\n",
    "\"\"\"     \n",
    "\n",
    "spark.sql(sql).show()\n",
    "spark.sql(sql).write.format(\"delta\").mode(\"overwrite\").save(\"/delta/result_vend_japao\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4120194890518507,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ERP - Desafio",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
